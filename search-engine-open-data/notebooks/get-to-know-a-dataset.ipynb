{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get To Know A Dataset: search.wilsonl.in Web Search Index Crawl + Text Embeddings\n",
    "\n",
    "This notebook demonstrates how to use the search engine dataset from AWS Open Data, containing 280 million web pages indexed with 3 billion neural embeddings.\n",
    "\n",
    "**Repository:** https://github.com/wilsonzlin/datasets/search-engine-open-data/\n",
    "\n",
    "## What This Covers\n",
    "\n",
    "- Representing natural language queries as vectors\n",
    "- Finding similar content using approximate nearest neighbor search (HNSW)\n",
    "- Working with sharded databases for scalability\n",
    "- Pinpointing the most relevant section within a page\n",
    "\n",
    "## About This Dataset\n",
    "\n",
    "A complete web search engine built from scratch:\n",
    "\n",
    "- **280 million web pages** crawled and parsed\n",
    "- **3 billion embeddings** generated using the sentence-transformers/multi-qa-mpnet-base-dot-v1 model\n",
    "- **Sharded architecture** designed to scale across hundreds of cores and terabytes of storage\n",
    "- **Dual-level indexing** for both page-level and sentence-level semantic search\n",
    "\n",
    "Read more about the original project: https://blog.wilsonl.in/search-engine\n",
    "\n",
    "**GitHub:** https://github.com/wilsonzlin/datasets/search-engine-open-data/\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "The dataset provides multiple ways to access the data:\n",
    "\n",
    "1. **Original RocksDB** (64 shards) - The native format with full data\n",
    "2. **Parquet export** - For analytics and bulk processing\n",
    "3. **Postcard binary** - Compact serialization for efficient loading\n",
    "4. **HNSW indices** - Both sharded (64) and combined (1) versions\n",
    "\n",
    "### Bucket Directory Tree\n",
    "\n",
    "```\n",
    "./rocksdb-shards/\n",
    "  shard-0/ ... shard-63/\n",
    "./kg-dbpedia/\n",
    "  vectors.hnsw\n",
    "  vectors.sqlite3\n",
    "  article-abstracts.sqlite3\n",
    "./kg-wikidata/\n",
    "  rocksdb/\n",
    "./resource-id/\n",
    "  rocksdb/\n",
    "./hnsw-combined/\n",
    "  statement.hnsw\n",
    "  block-mean.hnsw\n",
    "./export/\n",
    "  data.parquet\n",
    "  data-postcard/\n",
    "    uids.bin\n",
    "    offsets.bin\n",
    "    data.bin\n",
    "  statement_uid_base_to_resource_uid.arrow\n",
    "  statement_embeddings_msgpack.bin\n",
    "  block_embeddings_msgpack.bin\n",
    "  norm_doc_json_brotli.bin\n",
    "  statements_json_brotli.bin\n",
    "  source_brotli.bin\n",
    "  statement_labels_msgpack.bin\n",
    "  urls.txt\n",
    "  data.sql\n",
    "./hnsw-shards/\n",
    "  block-mean/shard-0/ ... shard-63/index.hnsw\n",
    "  statement/shard-0/ ... shard-63/index.hnsw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup\n",
    "\n",
    "First, let's install the necessary packages. We'll need:\n",
    "\n",
    "- **sentence-transformers** - To generate embeddings from text. This includes PyTorch and the transformer model.\n",
    "- **hnswlib** - Fast approximate nearest neighbor search using hierarchical navigable small world graphs.\n",
    "- **msgpack** - Efficient binary serialization format used throughout the dataset.\n",
    "- **pyarrow** - Apache Arrow and Parquet file reading for columnar data access.\n",
    "- **xxhash** - Fast non-cryptographic hash function, used for consistent sharding.\n",
    "- **brotli** - Compression algorithm used for text content (better compression than gzip).\n",
    "- **numpy** - Numerical operations on embeddings and vectors.\n",
    "- **python-rocksdb** - Python bindings for RocksDB, a high-performance embedded key-value store.\n",
    "- **duckdb** - In-process SQL OLAP database for Parquet analysis.\n",
    "- **pandas** - (Optional) For Parquet analysis examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers hnswlib msgpack pyarrow xxhash brotli numpy python-rocksdb duckdb pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mounting the Dataset\n",
    "\n",
    "The dataset is hosted on AWS S3 as part of the Open Data program. We'll mount it directly using `mount-s3`, which provides efficient read access without downloading everything locally.\n",
    "\n",
    "**Note:** Make sure you have `mount-s3` installed. On Amazon Linux 2023 or Ubuntu, you can install it with:\n",
    "\n",
    "```bash\n",
    "# Amazon Linux 2023\n",
    "sudo yum install mount-s3\n",
    "\n",
    "# Ubuntu\n",
    "wget https://s3.amazonaws.com/mountpoint-s3-release/latest/x86_64/mount-s3.deb\n",
    "sudo apt install ./mount-s3.deb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create mount point\n",
    "MOUNT_POINT = \"/mnt/search-engine\"\n",
    "!mkdir -p {MOUNT_POINT}\n",
    "\n",
    "# Mount the S3 bucket (read-only)\n",
    "!mount-s3 aws-opendata.wilsonl.in {MOUNT_POINT} --region us-east-1 --read-only\n",
    "\n",
    "# Set the data root\n",
    "DATA_ROOT = f\"{MOUNT_POINT}/search-engine\"\n",
    "\n",
    "# Verify the mount succeeded\n",
    "if not os.path.exists(DATA_ROOT):\n",
    "    raise RuntimeError(f\"Failed to mount dataset. Check that mount-s3 is installed and working.\")\n",
    "\n",
    "print(f\"Dataset successfully mounted at: {DATA_ROOT}\")\n",
    "print(\"Directory structure:\")\n",
    "for item in sorted(os.listdir(DATA_ROOT)):\n",
    "    item_path = os.path.join(DATA_ROOT, item)\n",
    "    if os.path.isdir(item_path):\n",
    "        print(f\"  {item}/\")\n",
    "    else:\n",
    "        print(f\"  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Data Architecture\n",
    "\n",
    "Before we dive into the code, let's understand how the data is organized.\n",
    "\n",
    "### Resource IDs and URLs\n",
    "\n",
    "Each web page (called a \"resource\") has:\n",
    "- A unique ID (`uid`) - a 64-bit integer\n",
    "- A normalized URL - the primary key\n",
    "- A `statement_uid_base` - the starting UID for statements (sentences) in that page\n",
    "\n",
    "The `resource-id` service maintains bidirectional mappings:\n",
    "```\n",
    "URL → Resource { uid, fetch_id, statement_uid_base }\n",
    "UID → URL\n",
    "Statement UID Base → URL\n",
    "```\n",
    "\n",
    "### RocksDB Sharding\n",
    "\n",
    "The data is split across 64 RocksDB shards using consistent hashing (XXH3). Each key has a prefix byte indicating its type, followed by the URL:\n",
    "\n",
    "**Key format:** `[prefix_byte][url]`\n",
    "\n",
    "| Prefix | Key Type | Value Format | Description |\n",
    "|--------|----------|--------------|-------------|\n",
    "| `0x01` | `Resource` | MessagePack | Basic metadata (title, state, HTTP status, etc.) |\n",
    "| `0x02` | `ResourceLinks` | MessagePack | Outbound links from this page |\n",
    "| `0x03` | `ResourceMeta` | MessagePack | OpenGraph and meta tags (og:description, etc.) |\n",
    "| `0x04` | `ResourceNormDoc` | JSON + Brotli | Normalized HTML structure |\n",
    "| `0x05` | `ResourceSource` | Brotli | Original HTML source |\n",
    "| `0x06` | `ResourceStatements` | JSON + Brotli | Sentence chunks with context |\n",
    "| `0x07` | `ResourceBlockEmbeddings` | MessagePack | Block-level embeddings (768-dim vectors) |\n",
    "| `0x08` | `ResourceStatementLabels` | MessagePack | Classification labels |\n",
    "| `0x09` | `ResourceStatementEmbeddings` | MessagePack | Statement-level embeddings (768-dim vectors) |\n",
    "\n",
    "### HNSW Vector Indices\n",
    "\n",
    "Two types of embeddings are indexed:\n",
    "- **block-mean**: Average of statements in each semantic block (paragraph-sized chunks)\n",
    "- **statement**: Individual sentence embeddings\n",
    "\n",
    "**Sharding history:** The HNSW indices were originally sharded into 64 pieces to fit within available RAM on individual machines and allow parallel querying across nodes. Each shard was built independently using consistent hashing to distribute vectors uniformly. Later, when exporting the dataset for public release, combined single-file indices were created for simplicity, though they require substantially more RAM to load.\n",
    "\n",
    "**Sizes:**\n",
    "- Sharded block-mean: 960 GB total (~15 GB per shard)\n",
    "- Sharded statement: 633 GB total (~10 GB per shard)\n",
    "- Combined block-mean: 1.1 TB\n",
    "- Combined statement: 767 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading the Embedding Model\n",
    "\n",
    "Load the embedding model used to create all 3 billion embeddings in the dataset: `multi-qa-mpnet-base-dot-v1`.\n",
    "\n",
    "This model:\n",
    "- Produces 768-dimensional vectors\n",
    "- Is optimized for semantic search and question answering\n",
    "- Uses dot product (inner product) as the similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "print(\"This will download the model on first run (~420MB).\")\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/multi-qa-mpnet-base-dot-v1')\n",
    "EMBEDDING_DIM = 768\n",
    "\n",
    "print(f\"Model loaded successfully. Embedding dimension: {EMBEDDING_DIM}\")\n",
    "\n",
    "# Test it with a simple query\n",
    "test_query = \"What is semantic search?\"\n",
    "test_embedding = model.encode(test_query, normalize_embeddings=True)\n",
    "print(f\"Test query: '{test_query}'\")\n",
    "print(f\"Embedding shape: {test_embedding.shape}\")\n",
    "print(f\"First 5 dimensions: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading Sharded HNSW Indices\n",
    "\n",
    "Now we'll load the HNSW (Hierarchical Navigable Small World) indices. For production use, the dataset uses 64 shards that can be queried in parallel.\n",
    "\n",
    "We'll load all 64 shards of the block-mean index, which represents page-level semantic embeddings. Each shard contains approximately 15 GB of vectors.\n",
    "\n",
    "**Memory requirement:** ~960 GB total for all block-mean shards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hnswlib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "SHARD_COUNT = 64\n",
    "\n",
    "def load_hnsw_index(path: str, dim: int = EMBEDDING_DIM) -> hnswlib.Index:\n",
    "    \"\"\"\n",
    "    Load an HNSW index from disk.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to the .hnsw file\n",
    "        dim: Embedding dimensionality (default: 768)\n",
    "    \n",
    "    Returns:\n",
    "        Loaded HNSW index ready for queries\n",
    "    \"\"\"\n",
    "    index = hnswlib.Index(space='ip', dim=dim)  # 'ip' = inner product\n",
    "    index.load_index(path)\n",
    "    return index\n",
    "\n",
    "print(\"Loading 64 shards of block-mean HNSW indices...\")\n",
    "print(\"This will load approximately 960 GB into memory.\")\n",
    "print(\"Progress:\")\n",
    "\n",
    "block_mean_shards = []\n",
    "for i in range(SHARD_COUNT):\n",
    "    shard_path = f\"{DATA_ROOT}/hnsw-shards/block-mean/shard-{i}/index.hnsw\"\n",
    "    index = load_hnsw_index(shard_path)\n",
    "    block_mean_shards.append(index)\n",
    "    \n",
    "    if (i + 1) % 16 == 0:\n",
    "        print(f\"  Loaded shards 0-{i} ({i+1}/{SHARD_COUNT})\")\n",
    "\n",
    "total_vectors = sum(shard.get_current_count() for shard in block_mean_shards)\n",
    "print(f\"All block-mean shards loaded: {total_vectors:,} total vectors\")\n",
    "\n",
    "# Initialize thread pool for parallel queries\n",
    "query_executor = ThreadPoolExecutor(max_workers=64)\n",
    "print(\"Thread pool initialized for parallel queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statement shards (sentence-level embeddings)\n",
    "print(\"Loading 64 shards of statement HNSW indices...\")\n",
    "print(\"This will load approximately 633 GB into memory.\")\n",
    "print(\"Progress:\")\n",
    "\n",
    "statement_shards = []\n",
    "for i in range(SHARD_COUNT):\n",
    "    shard_path = f\"{DATA_ROOT}/hnsw-shards/statement/shard-{i}/index.hnsw\"\n",
    "    index = load_hnsw_index(shard_path)\n",
    "    statement_shards.append(index)\n",
    "    \n",
    "    if (i + 1) % 16 == 0:\n",
    "        print(f\"  Loaded shards 0-{i} ({i+1}/{SHARD_COUNT})\")\n",
    "\n",
    "total_statements = sum(shard.get_current_count() for shard in statement_shards)\n",
    "print(f\"All statement shards loaded: {total_statements:,} total vectors\")\n",
    "print(f\"Total memory used by indices: ~1.6 TB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Querying Sharded HNSW Indices\n",
    "\n",
    "With sharded indices, query all shards in parallel and merge the results.\n",
    "\n",
    "**Important distinction:**\n",
    "- Block-mean index: IDs are resource UIDs directly\n",
    "- Statement index: IDs are statement UIDs, which must be mapped to resource UIDs via statement_uid_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sharded_hnsw(shards: list, query_vec: np.ndarray, k: int, executor: ThreadPoolExecutor) -> list:\n",
    "    \"\"\"\n",
    "    Query all HNSW shards in parallel and return top k results.\n",
    "    \n",
    "    Args:\n",
    "        shards: List of HNSW index shards\n",
    "        query_vec: Query embedding vector\n",
    "        k: Number of results to return\n",
    "        executor: ThreadPoolExecutor for parallel queries\n",
    "    \n",
    "    Returns:\n",
    "        List of (uid, distance) tuples sorted by distance\n",
    "    \"\"\"\n",
    "    def query_shard(shard):\n",
    "        labels, distances = shard.knn_query(query_vec, k=k)\n",
    "        return list(zip(labels[0], distances[0]))\n",
    "    \n",
    "    # Query all shards in parallel\n",
    "    results = list(executor.map(query_shard, shards))\n",
    "    \n",
    "    # Merge results\n",
    "    all_results = []\n",
    "    for shard_results in results:\n",
    "        all_results.extend(shard_results)\n",
    "    \n",
    "    # Sort by distance and take top k\n",
    "    all_results.sort(key=lambda x: x[1])\n",
    "    return all_results[:k]\n",
    "\n",
    "# Test it\n",
    "test_query_vec = model.encode(\"distributed systems\", normalize_embeddings=True).astype('float32')\n",
    "test_results = query_sharded_hnsw(block_mean_shards, test_query_vec, k=10, executor=query_executor)\n",
    "\n",
    "print(\"Test query: 'distributed systems'\")\n",
    "print(f\"Found {len(test_results)} results\")\n",
    "print(\"Top 3 results:\")\n",
    "for i, (uid, distance) in enumerate(test_results[:3], 1):\n",
    "    similarity = 1 - distance\n",
    "    print(f\"  {i}. UID {uid}: similarity {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Working with RocksDB Shards\n",
    "\n",
    "Now let's set up access to the RocksDB shards using the raw Python bindings.\n",
    "\n",
    "### Sharding Logic\n",
    "\n",
    "To find which shard contains a key, we use XXH3 hash:\n",
    "```python\n",
    "shard_number = xxh3_64(key) % 64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rocksdb\n",
    "import xxhash\n",
    "\n",
    "# Key prefixes matching RocksDbKey enum\n",
    "class KeyPrefix:\n",
    "    RESOURCE = 1\n",
    "    RESOURCE_LINKS = 2\n",
    "    RESOURCE_META = 3\n",
    "    RESOURCE_NORM_DOC = 4\n",
    "    RESOURCE_SOURCE = 5\n",
    "    RESOURCE_STATEMENTS = 6\n",
    "    RESOURCE_BLOCK_EMBEDDINGS = 7\n",
    "    RESOURCE_STATEMENT_LABELS = 8\n",
    "    RESOURCE_STATEMENT_EMBEDDINGS = 9\n",
    "\n",
    "def build_key(prefix: int, url: str) -> bytes:\n",
    "    \"\"\"Build a RocksDB key: [prefix_byte][url]\"\"\"\n",
    "    return bytes([prefix]) + url.encode('utf-8')\n",
    "\n",
    "def get_shard_for_key(key: bytes) -> int:\n",
    "    \"\"\"Determine which shard a key belongs to using XXH3 hash.\"\"\"\n",
    "    return xxhash.xxh64(key).intdigest() % SHARD_COUNT\n",
    "\n",
    "print(\"Key building functions defined.\")\n",
    "print(\"Example key for URL 'https://example.com':\")\n",
    "example_key = build_key(KeyPrefix.RESOURCE, \"https://example.com\")\n",
    "print(f\"  Raw bytes: {example_key[:20]}...\")\n",
    "print(f\"  Shard: {get_shard_for_key(example_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure RocksDB options matching the original implementation\n",
    "print(\"Opening RocksDB shards...\")\n",
    "print(\"This will open 64 read-only database connections.\")\n",
    "\n",
    "opts = rocksdb.Options()\n",
    "opts.create_if_missing = False\n",
    "opts.max_open_files = 300\n",
    "\n",
    "# BlobDB settings for large values\n",
    "opts.enable_blob_files = True\n",
    "opts.min_blob_size = 0\n",
    "\n",
    "# Cache and performance settings\n",
    "block_cache = rocksdb.LRUCache(512 * 1024 * 1024)  # 512 MB cache\n",
    "block_opts = rocksdb.BlockBasedTableFactory(\n",
    "    block_cache=block_cache,\n",
    "    filter_policy=rocksdb.BloomFilterPolicy(10),\n",
    "    block_size=16 * 1024,\n",
    ")\n",
    "opts.table_factory = block_opts\n",
    "\n",
    "rocksdb_shards = []\n",
    "for i in range(SHARD_COUNT):\n",
    "    shard_path = f\"{DATA_ROOT}/rocksdb-shards/shard-{i}\"\n",
    "    db = rocksdb.DB(shard_path, opts, read_only=True)\n",
    "    rocksdb_shards.append(db)\n",
    "    \n",
    "    if (i + 1) % 16 == 0:\n",
    "        print(f\"  Opened shards 0-{i}\")\n",
    "\n",
    "print(f\"All {len(rocksdb_shards)} shards opened successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: The Resource ID Service\n",
    "\n",
    "The resource ID service provides mappings between URLs and numeric IDs. This is crucial for mapping statement UIDs back to their parent resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack\n",
    "import bisect\n",
    "\n",
    "# Open the resource-id RocksDB\n",
    "resource_id_db_path = f\"{DATA_ROOT}/resource-id/rocksdb\"\n",
    "resource_id_opts = rocksdb.Options()\n",
    "resource_id_opts.create_if_missing = False\n",
    "resource_id_db = rocksdb.DB(resource_id_db_path, resource_id_opts, read_only=True)\n",
    "\n",
    "print(f\"Resource ID database opened: {resource_id_db_path}\")\n",
    "\n",
    "# Key prefixes for resource-id service\n",
    "class ResourceIdKeyPrefix:\n",
    "    RESOURCE_URL = 1   # URL -> Resource\n",
    "    RESOURCE_UID = 2   # UID -> URL  \n",
    "    STATEMENT_UID_BASE = 3  # Statement UID Base -> URL\n",
    "\n",
    "def k_resource_url(url: str) -> bytes:\n",
    "    \"\"\"Build key to lookup Resource by URL.\"\"\"\n",
    "    return bytes([ResourceIdKeyPrefix.RESOURCE_URL]) + url.encode('utf-8')\n",
    "\n",
    "def k_resource_uid(uid: int) -> bytes:\n",
    "    \"\"\"Build key to lookup URL by resource UID (big-endian for ordering).\"\"\"\n",
    "    return bytes([ResourceIdKeyPrefix.RESOURCE_UID]) + uid.to_bytes(8, byteorder='big')\n",
    "\n",
    "def k_statement_uid_base(uid: int) -> bytes:\n",
    "    \"\"\"Build key to lookup URL by statement UID base (big-endian for ordering).\"\"\"\n",
    "    return bytes([ResourceIdKeyPrefix.STATEMENT_UID_BASE]) + uid.to_bytes(8, byteorder='big')\n",
    "\n",
    "def get_resource_by_url(url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Look up resource metadata by URL.\n",
    "    Returns: {uid: int, fetch_id: int, statement_uid_base: Optional[int]}\n",
    "    \"\"\"\n",
    "    key = k_resource_url(url)\n",
    "    value = resource_id_db.get(key)\n",
    "    if value is None:\n",
    "        return None\n",
    "    return msgpack.unpackb(value, raw=False)\n",
    "\n",
    "def get_url_by_uid(uid: int) -> str:\n",
    "    \"\"\"Look up URL by resource UID.\"\"\"\n",
    "    key = k_resource_uid(uid)\n",
    "    value = resource_id_db.get(key)\n",
    "    if value is None:\n",
    "        return None\n",
    "    return value.decode('utf-8')\n",
    "\n",
    "def get_url_by_statement_uid(statement_uid: int) -> str:\n",
    "    \"\"\"\n",
    "    Look up URL by statement UID.\n",
    "    Uses RocksDB's seek to find the largest statement_uid_base <= statement_uid.\n",
    "    \"\"\"\n",
    "    # Seek to the statement UID key\n",
    "    seek_key = k_statement_uid_base(statement_uid)\n",
    "    \n",
    "    # Create iterator starting at this key\n",
    "    it = resource_id_db.iterkeys()\n",
    "    it.seek(seek_key)\n",
    "    \n",
    "    # Get current key or move to previous if we're past the target\n",
    "    try:\n",
    "        key = next(it)\n",
    "        # Check if this key is for statement_uid_base\n",
    "        if key[0] != ResourceIdKeyPrefix.STATEMENT_UID_BASE:\n",
    "            # We're past all statement UIDs, go back\n",
    "            it.seek(seek_key)\n",
    "            it.prev()\n",
    "            key = it.key()\n",
    "        else:\n",
    "            # Check if the key is exactly our target or less\n",
    "            key_uid = int.from_bytes(key[1:], byteorder='big')\n",
    "            if key_uid > statement_uid:\n",
    "                # We need the previous key\n",
    "                it.prev()\n",
    "                key = it.key()\n",
    "    except StopIteration:\n",
    "        return None\n",
    "    \n",
    "    # Verify this is a statement_uid_base key\n",
    "    if key[0] != ResourceIdKeyPrefix.STATEMENT_UID_BASE:\n",
    "        return None\n",
    "    \n",
    "    value = resource_id_db.get(key)\n",
    "    if value is None:\n",
    "        return None\n",
    "    return value.decode('utf-8')\n",
    "\n",
    "print(\"Resource ID lookup functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Data Access Functions\n",
    "\n",
    "Helper functions to fetch different types of data from the RocksDB shards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brotli\n",
    "import json\n",
    "\n",
    "def get_from_rocksdb(url: str, key_prefix: int):\n",
    "    \"\"\"\n",
    "    Fetch a value from the appropriate RocksDB shard.\n",
    "    Returns raw bytes or None if not found.\n",
    "    \"\"\"\n",
    "    key = build_key(key_prefix, url)\n",
    "    shard_no = get_shard_for_key(key)\n",
    "    return rocksdb_shards[shard_no].get(key)\n",
    "\n",
    "def get_resource_metadata(url: str) -> dict:\n",
    "    \"\"\"Get resource metadata (title, state, HTTP status, etc.).\"\"\"\n",
    "    data = get_from_rocksdb(url, KeyPrefix.RESOURCE)\n",
    "    if data is None:\n",
    "        return None\n",
    "    return msgpack.unpackb(data, raw=False)\n",
    "\n",
    "def get_resource_meta_tags(url: str) -> dict:\n",
    "    \"\"\"Get OpenGraph and meta tags.\"\"\"\n",
    "    data = get_from_rocksdb(url, KeyPrefix.RESOURCE_META)\n",
    "    if data is None:\n",
    "        return {}\n",
    "    return msgpack.unpackb(data, raw=False)\n",
    "\n",
    "def get_statements(url: str) -> dict:\n",
    "    \"\"\"Get parsed statements (sentence chunks) with context.\"\"\"\n",
    "    data = get_from_rocksdb(url, KeyPrefix.RESOURCE_STATEMENTS)\n",
    "    if data is None:\n",
    "        return None\n",
    "    json_str = brotli.decompress(data).decode('utf-8')\n",
    "    return json.loads(json_str)\n",
    "\n",
    "def get_block_embeddings(url: str) -> dict:\n",
    "    \"\"\"Get block-level embeddings.\"\"\"\n",
    "    data = get_from_rocksdb(url, KeyPrefix.RESOURCE_BLOCK_EMBEDDINGS)\n",
    "    if data is None:\n",
    "        return None\n",
    "    return msgpack.unpackb(data, raw=False)\n",
    "\n",
    "def get_normalized_doc(url: str) -> dict:\n",
    "    \"\"\"Get normalized HTML structure.\"\"\"\n",
    "    data = get_from_rocksdb(url, KeyPrefix.RESOURCE_NORM_DOC)\n",
    "    if data is None:\n",
    "        return None\n",
    "    json_str = brotli.decompress(data).decode('utf-8')\n",
    "    return json.loads(json_str)\n",
    "\n",
    "print(\"Data access functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Semantic Search Pipeline\n",
    "\n",
    "Complete semantic search implementation.\n",
    "\n",
    "The workflow:\n",
    "1. Embed the query\n",
    "2. Search sharded HNSW indices for similar vectors\n",
    "3. Map statement UIDs to resource UIDs (for statement results)\n",
    "4. Fetch page data from RocksDB\n",
    "5. Rank and return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, k: int = 10):\n",
    "    \"\"\"\n",
    "    Perform semantic search for a natural language query.\n",
    "    \n",
    "    Args:\n",
    "        query: Natural language search query\n",
    "        k: Number of results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of search results with metadata\n",
    "    \"\"\"\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Embed the query\n",
    "    print(\"[1/5] Generating query embedding...\")\n",
    "    query_vec = model.encode(query, normalize_embeddings=True).astype('float32')\n",
    "    print(f\"      Generated {len(query_vec)}-dimensional vector\")\n",
    "    \n",
    "    # Step 2: Search sharded HNSW indices\n",
    "    print(\"[2/5] Searching HNSW indices across 64 shards...\")\n",
    "    block_results = query_sharded_hnsw(block_mean_shards, query_vec, k=100, executor=query_executor)\n",
    "    stmt_results = query_sharded_hnsw(statement_shards, query_vec, k=32, executor=query_executor)\n",
    "    \n",
    "    print(f\"      Block-mean: {len(block_results)} candidates\")\n",
    "    print(f\"      Statement: {len(stmt_results)} candidates\")\n",
    "    \n",
    "    # Step 3: Map UIDs to resources\n",
    "    print(\"[3/5] Mapping UIDs to resource URLs...\")\n",
    "    \n",
    "    # Block results have resource UIDs directly\n",
    "    results = {}\n",
    "    for resource_uid, distance in block_results:\n",
    "        if resource_uid not in results:\n",
    "            results[resource_uid] = {\n",
    "                'uid': int(resource_uid),\n",
    "                'block_distance': float(distance),\n",
    "                'block_similarity': float(1 - distance),\n",
    "                'statement_distances': []\n",
    "            }\n",
    "    \n",
    "    # Statement results have statement UIDs - need to map to resource UIDs\n",
    "    for statement_uid, distance in stmt_results:\n",
    "        url = get_url_by_statement_uid(int(statement_uid))\n",
    "        if url:\n",
    "            resource_info = get_resource_by_url(url)\n",
    "            if resource_info:\n",
    "                resource_uid = resource_info['uid']\n",
    "                if resource_uid not in results:\n",
    "                    results[resource_uid] = {\n",
    "                        'uid': resource_uid,\n",
    "                        'block_distance': float('inf'),\n",
    "                        'block_similarity': 0.0,\n",
    "                        'statement_distances': []\n",
    "                    }\n",
    "                results[resource_uid]['statement_distances'].append(float(distance))\n",
    "    \n",
    "    # Convert to list and sort by best distance\n",
    "    results = list(results.values())\n",
    "    results.sort(key=lambda r: min(r['block_distance'], \n",
    "                                   min(r['statement_distances']) if r['statement_distances'] else float('inf')))\n",
    "    results = results[:k]\n",
    "    \n",
    "    # Map resource UIDs to URLs\n",
    "    for result in results:\n",
    "        url = get_url_by_uid(result['uid'])\n",
    "        result['url'] = url\n",
    "    \n",
    "    results = [r for r in results if r['url'] is not None]\n",
    "    print(f\"      Resolved {len(results)} URLs\")\n",
    "    \n",
    "    # Step 4: Fetch metadata\n",
    "    print(\"[4/5] Fetching page metadata...\")\n",
    "    for result in results:\n",
    "        metadata = get_resource_metadata(result['url'])\n",
    "        if metadata:\n",
    "            result['title'] = metadata.get('title')\n",
    "            result['state'] = metadata.get('state')\n",
    "        \n",
    "        meta_tags = get_resource_meta_tags(result['url'])\n",
    "        result['description'] = meta_tags.get('og:description', '')\n",
    "    \n",
    "    print(\"      Metadata fetched\")\n",
    "    \n",
    "    # Step 5: Ranking complete\n",
    "    print(\"[5/5] Ranking complete\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Try it!\n",
    "results = semantic_search(\"how does semantic search work with neural networks\", k=5)\n",
    "\n",
    "# Display results\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. {r['title'] or '(No title)'}\")\n",
    "    print(f\"   URL: {r['url']}\")\n",
    "    print(f\"   Similarity: {r['block_similarity']:.4f}\")\n",
    "    if r['description']:\n",
    "        desc = r['description'][:150]\n",
    "        print(f\"   {desc}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Finding the Most Relevant Section\n",
    "\n",
    "Use block embeddings to pinpoint the exact section of a page that's most relevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_section(url: str, query: str):\n",
    "    \"\"\"\n",
    "    Find the most relevant section within a page for a query.\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing page: {url}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_vec = model.encode(query, normalize_embeddings=True).astype('float32')\n",
    "    \n",
    "    # Get block embeddings\n",
    "    print(\"Fetching block embeddings...\")\n",
    "    block_data = get_block_embeddings(url)\n",
    "    if not block_data or not block_data.get('blocks'):\n",
    "        print(\"  No block embeddings found for this page.\")\n",
    "        return\n",
    "    \n",
    "    blocks = block_data['blocks']\n",
    "    print(f\"  Found {len(blocks)} blocks\")\n",
    "    \n",
    "    # Get statements (text chunks)\n",
    "    print(\"Fetching statements...\")\n",
    "    statements_data = get_statements(url)\n",
    "    if not statements_data:\n",
    "        print(\"  No statements found for this page.\")\n",
    "        return\n",
    "    \n",
    "    statements = statements_data['statements']\n",
    "    print(f\"  Found {len(statements)} statements\")\n",
    "    \n",
    "    # Find most similar block\n",
    "    print(\"Computing similarity scores...\")\n",
    "    best_score = -float('inf')\n",
    "    best_block_idx = 0\n",
    "    \n",
    "    for i, block in enumerate(blocks):\n",
    "        embedding_bytes = block['embedding']\n",
    "        embedding = np.frombuffer(embedding_bytes, dtype=np.float32)\n",
    "        score = np.dot(query_vec, embedding)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_block_idx = i\n",
    "    \n",
    "    best_block = blocks[best_block_idx]\n",
    "    start_idx = best_block['statement_start_index']\n",
    "    \n",
    "    print(f\"  Most relevant block: #{best_block_idx}\")\n",
    "    print(f\"  Similarity score: {best_score:.4f}\")\n",
    "    print(f\"  Starts at statement {start_idx}\")\n",
    "    \n",
    "    # Show the relevant text\n",
    "    print(\"Most relevant section:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    end_idx = min(start_idx + 5, len(statements))\n",
    "    for i in range(start_idx, end_idx):\n",
    "        text = statements[i]['text']\n",
    "        print(f\"{text}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Try it with the first result from our search\n",
    "if results:\n",
    "    find_relevant_section(results[0]['url'], \"how does semantic search work with neural networks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Analytics with Parquet + DuckDB\n",
    "\n",
    "The dataset is also available in Parquet format for analytics and bulk processing. DuckDB provides a SQL interface for analyzing the data without loading it all into memory.\n",
    "\n",
    "Analytical query examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Connect to DuckDB\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "parquet_path = f\"{DATA_ROOT}/export/data.parquet\"\n",
    "print(f\"Using Parquet file: {parquet_path}\")\n",
    "print(\"DuckDB can query Parquet files directly without loading into memory.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Count pages by HTTP status\n",
    "print(\"Pages by HTTP status code:\")\n",
    "print(\"=\" * 80)\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        http_status,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM read_parquet('{parquet_path}')\n",
    "    WHERE http_status IS NOT NULL\n",
    "    GROUP BY http_status\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "print(result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Top domains by page count\n",
    "print(\"Top 20 domains by number of indexed pages:\")\n",
    "print(\"=\" * 80)\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        url_hostname,\n",
    "        COUNT(*) as page_count\n",
    "    FROM read_parquet('{parquet_path}')\n",
    "    WHERE url_hostname IS NOT NULL\n",
    "    GROUP BY url_hostname\n",
    "    ORDER BY page_count DESC\n",
    "    LIMIT 20\n",
    "\"\"\").fetchdf()\n",
    "print(result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Pages by top-level domain extension\n",
    "print(\"Pages by TLD:\")\n",
    "print(\"=\" * 80)\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        SPLIT_PART(url_hostname, '.', -1) as tld,\n",
    "        COUNT(*) as count\n",
    "    FROM read_parquet('{parquet_path}')\n",
    "    WHERE url_hostname IS NOT NULL\n",
    "    GROUP BY tld\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 15\n",
    "\"\"\").fetchdf()\n",
    "print(result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Pages by file extension\n",
    "print(\"Pages by file extension:\")\n",
    "print(\"=\" * 80)\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN url_path_ext = '' THEN '(no extension)'\n",
    "            ELSE url_path_ext\n",
    "        END as extension,\n",
    "        COUNT(*) as count\n",
    "    FROM read_parquet('{parquet_path}')\n",
    "    GROUP BY extension\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 15\n",
    "\"\"\").fetchdf()\n",
    "print(result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Distribution of resource states\n",
    "print(\"Resource state distribution:\")\n",
    "print(\"=\" * 80)\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        state,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM read_parquet('{parquet_path}')\n",
    "    WHERE state IS NOT NULL\n",
    "    GROUP BY state\n",
    "    ORDER BY count DESC\n",
    "\"\"\").fetchdf()\n",
    "print(result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Find pages with specific keywords in title (case-insensitive)\n",
    "keyword = \"machine learning\"\n",
    "print(f\"Pages with '{keyword}' in title:\")\n",
    "print(\"=\" * 80)\n",
    "result = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        title,\n",
    "        url_hostname,\n",
    "        url\n",
    "    FROM read_parquet('{parquet_path}')\n",
    "    WHERE LOWER(title) LIKE LOWER('%{keyword}%')\n",
    "    LIMIT 10\n",
    "\"\"\").fetchdf()\n",
    "print(result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Simple Approach - Combined HNSW + Postcard\n",
    "\n",
    "For simpler use cases or when you have sufficient RAM, you can use the combined HNSW indices with the Postcard binary format for fast direct lookups.\n",
    "\n",
    "This approach is simpler because:\n",
    "- Single index file instead of 64 shards\n",
    "- No need to query in parallel and merge\n",
    "- Direct UID to data mapping via Postcard\n",
    "\n",
    "**Requirements:**\n",
    "- Combined block-mean: 1.1 TB RAM\n",
    "- Combined statement: 767 GB RAM\n",
    "- Total: ~1.9 TB RAM\n",
    "\n",
    "### Loading Combined HNSW Indices\n",
    "\n",
    "Let's load the single combined indices instead of the 64 shards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading combined HNSW indices...\")\n",
    "print(\"This requires approximately 1.9 TB of RAM.\")\n",
    "print(\"This is simpler than sharded approach but needs more memory.\")\n",
    "print()\n",
    "\n",
    "# Load combined block-mean index\n",
    "print(\"Loading combined block-mean index (1.1 TB)...\")\n",
    "combined_block_mean = load_hnsw_index(f\"{DATA_ROOT}/hnsw-combined/block-mean.hnsw\")\n",
    "print(f\"  Loaded: {combined_block_mean.get_current_count():,} vectors\")\n",
    "\n",
    "# Load combined statement index\n",
    "print(\"Loading combined statement index (767 GB)...\")\n",
    "combined_statement = load_hnsw_index(f\"{DATA_ROOT}/hnsw-combined/statement.hnsw\")\n",
    "print(f\"  Loaded: {combined_statement.get_current_count():,} vectors\")\n",
    "\n",
    "print(\"Combined indices loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Search with Combined Indices\n",
    "\n",
    "With combined indices, searching is much simpler - just query once, no parallel execution or merging needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_semantic_search(query: str, k: int = 10):\n",
    "    \"\"\"\n",
    "    Simplified semantic search using combined indices.\n",
    "    Much simpler than the sharded approach.\n",
    "    \"\"\"\n",
    "    print(f\"Query: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Embed query\n",
    "    query_vec = model.encode(query, normalize_embeddings=True).astype('float32')\n",
    "    \n",
    "    # Query combined indices directly (no sharding, no parallel execution)\n",
    "    print(\"Querying combined indices...\")\n",
    "    block_labels, block_distances = combined_block_mean.knn_query(query_vec, k=100)\n",
    "    stmt_labels, stmt_distances = combined_statement.knn_query(query_vec, k=32)\n",
    "    \n",
    "    # Collect results (same logic as before)\n",
    "    results = {}\n",
    "    \n",
    "    # Block results have resource UIDs directly\n",
    "    for resource_uid, distance in zip(block_labels[0], block_distances[0]):\n",
    "        if resource_uid not in results:\n",
    "            results[resource_uid] = {\n",
    "                'uid': int(resource_uid),\n",
    "                'block_distance': float(distance),\n",
    "                'block_similarity': float(1 - distance),\n",
    "                'statement_distances': []\n",
    "            }\n",
    "    \n",
    "    # Statement results need mapping\n",
    "    for statement_uid, distance in zip(stmt_labels[0], stmt_distances[0]):\n",
    "        url = get_url_by_statement_uid(int(statement_uid))\n",
    "        if url:\n",
    "            resource_info = get_resource_by_url(url)\n",
    "            if resource_info:\n",
    "                resource_uid = resource_info['uid']\n",
    "                if resource_uid not in results:\n",
    "                    results[resource_uid] = {\n",
    "                        'uid': resource_uid,\n",
    "                        'block_distance': float('inf'),\n",
    "                        'block_similarity': 0.0,\n",
    "                        'statement_distances': []\n",
    "                    }\n",
    "                results[resource_uid]['statement_distances'].append(float(distance))\n",
    "    \n",
    "    # Sort and fetch metadata\n",
    "    results = list(results.values())\n",
    "    results.sort(key=lambda r: min(r['block_distance'], \n",
    "                                   min(r['statement_distances']) if r['statement_distances'] else float('inf')))\n",
    "    results = results[:k]\n",
    "    \n",
    "    # Map to URLs and fetch metadata\n",
    "    for result in results:\n",
    "        url = get_url_by_uid(result['uid'])\n",
    "        result['url'] = url\n",
    "        if url:\n",
    "            metadata = get_resource_metadata(url)\n",
    "            if metadata:\n",
    "                result['title'] = metadata.get('title')\n",
    "            meta_tags = get_resource_meta_tags(url)\n",
    "            result['description'] = meta_tags.get('og:description', '')\n",
    "    \n",
    "    results = [r for r in results if r['url'] is not None]\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the simple approach\n",
    "simple_results = simple_semantic_search(\"machine learning fundamentals\", k=5)\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for i, r in enumerate(simple_results, 1):\n",
    "    print(f\"{i}. {r['title'] or '(No title)'}\")\n",
    "    print(f\"   Similarity: {r['block_similarity']:.4f}\")\n",
    "    print(f\"   URL: {r['url']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postcard Implementation\n",
    "\n",
    "The Postcard format is optimized for Rust. Below is a Rust implementation that you can compile as a Python extension using PyO3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```rust\n",
    "// Cargo.toml dependencies:\n",
    "// pyo3 = { version = \"0.20\", features = [\"extension-module\"] }\n",
    "// postcard = \"1.0\"\n",
    "// serde = { version = \"1.0\", features = [\"derive\"] }\n",
    "// dashmap = \"5.5\"\n",
    "// bytemuck = \"1.14\"\n",
    "\n",
    "use pyo3::prelude::*;\n",
    "use std::fs::File;\n",
    "use std::io::{Read, Seek, SeekFrom};\n",
    "use dashmap::DashMap;\n",
    "use serde::{Deserialize, Serialize};\n",
    "use bytemuck::cast_slice;\n",
    "\n",
    "#[derive(Deserialize, Serialize, Clone)]\n",
    "pub struct ExportedResourcePostcardRow {\n",
    "    pub url: String,\n",
    "    pub uid: u64,\n",
    "    pub fetch_id: u64,\n",
    "    pub statement_uid_base: Option<u64>,\n",
    "    pub http_status: Option<u16>,\n",
    "    pub title: Option<String>,\n",
    "    pub icon_url: Option<String>,\n",
    "    // Offsets for external blob files\n",
    "    pub norm_doc_json_brotli_offset: Option<u64>,\n",
    "    pub norm_doc_json_brotli_size: u64,\n",
    "    pub source_brotli_offset: Option<u64>,\n",
    "    pub source_brotli_size: u64,\n",
    "    pub statements_json_brotli_offset: Option<u64>,\n",
    "    pub statements_json_brotli_size: u64,\n",
    "    pub block_embeddings_msgpack_offset: Option<u64>,\n",
    "    pub block_embeddings_msgpack_size: u64,\n",
    "    pub statement_embeddings_msgpack_offset: Option<u64>,\n",
    "    pub statement_embeddings_msgpack_size: u64,\n",
    "}\n",
    "\n",
    "#[pyclass]\n",
    "struct PostcardData {\n",
    "    uid_to_row: DashMap<u64, usize>,\n",
    "    row_to_offset: DashMap<usize, u64>,\n",
    "    data_path: String,\n",
    "    data_len: u64,\n",
    "}\n",
    "\n",
    "#[pymethods]\n",
    "impl PostcardData {\n",
    "    #[new]\n",
    "    fn new(folder: &str) -> PyResult<Self> {\n",
    "        let folder_path = std::path::Path::new(folder);\n",
    "        \n",
    "        let mut uids_file = File::open(folder_path.join(\"uids.bin\"))?;\n",
    "        let mut uids_bytes = Vec::new();\n",
    "        uids_file.read_to_end(&mut uids_bytes)?;\n",
    "        let uids: &[u64] = cast_slice(&uids_bytes);\n",
    "        \n",
    "        let mut offsets_file = File::open(folder_path.join(\"offsets.bin\"))?;\n",
    "        let mut offsets_bytes = Vec::new();\n",
    "        offsets_file.read_to_end(&mut offsets_bytes)?;\n",
    "        let offsets: &[u64] = cast_slice(&offsets_bytes);\n",
    "        \n",
    "        let uid_to_row = DashMap::new();\n",
    "        let row_to_offset = DashMap::new();\n",
    "        \n",
    "        for (row, (&uid, &offset)) in uids.iter().zip(offsets.iter()).enumerate() {\n",
    "            uid_to_row.insert(uid, row);\n",
    "            row_to_offset.insert(row, offset);\n",
    "        }\n",
    "        \n",
    "        let data_path = folder_path.join(\"data.bin\").to_string_lossy().to_string();\n",
    "        let data_len = std::fs::metadata(&data_path)?.len();\n",
    "        \n",
    "        Ok(Self { uid_to_row, row_to_offset, data_path, data_len })\n",
    "    }\n",
    "    \n",
    "    fn get(&self, uid: u64) -> PyResult<ExportedResourcePostcardRow> {\n",
    "        let row = *self.uid_to_row.get(&uid)\n",
    "            .ok_or_else(|| pyo3::exceptions::PyKeyError::new_err(\"UID not found\"))?;\n",
    "        \n",
    "        let offset = *self.row_to_offset.get(&row).unwrap();\n",
    "        let next_offset = self.row_to_offset.get(&(row + 1))\n",
    "            .map(|r| *r)\n",
    "            .unwrap_or(self.data_len);\n",
    "        \n",
    "        let mut file = File::open(&self.data_path)?;\n",
    "        file.seek(SeekFrom::Start(offset))?;\n",
    "        let mut buffer = vec![0u8; (next_offset - offset) as usize];\n",
    "        file.read_exact(&mut buffer)?;\n",
    "        \n",
    "        postcard::from_bytes(&buffer)\n",
    "            .map_err(|e| pyo3::exceptions::PyValueError::new_err(format!(\"{}\", e)))\n",
    "    }\n",
    "}\n",
    "\n",
    "#[pymodule]\n",
    "fn postcard_reader(_py: Python, m: &PyModule) -> PyResult<()> {\n",
    "    m.add_class::<PostcardData>()?;\n",
    "    Ok(())\n",
    "}\n",
    "```\n",
    "\n",
    "Build with `maturin`:\n",
    "```bash\n",
    "pip install maturin\n",
    "maturin develop --release\n",
    "```\n",
    "\n",
    "Use in Python:\n",
    "```python\n",
    "import postcard_reader\n",
    "postcard = postcard_reader.PostcardData(f\"{DATA_ROOT}/export/data-postcard\")\n",
    "resource = postcard.get(12345)\n",
    "print(resource.title)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "1. Mounting and accessing the AWS Open Data search engine dataset\n",
    "2. Generating embeddings for natural language queries\n",
    "3. Searching billions of vectors using sharded HNSW indices\n",
    "4. Working with sharded RocksDB using the exact same schema as the original implementation\n",
    "5. Mapping between resource UIDs, statement UIDs, and URLs\n",
    "6. Finding the most relevant sections within pages\n",
    "7. Performing analytical queries using DuckDB and Parquet\n",
    "8. Understanding the Postcard format for efficient data access\n",
    "\n",
    "### Two Approaches\n",
    "\n",
    "**Sharded HNSW (Parts 2-8):**\n",
    "- Uses ~1.6 TB RAM\n",
    "- Queries 64 shards in parallel\n",
    "- More complex but scales better\n",
    "- Production approach\n",
    "\n",
    "**Combined HNSW + Postcard (Part 10):**\n",
    "- Uses ~1.9 TB RAM\n",
    "- Single index, simpler queries\n",
    "- Good for experimentation\n",
    "- Requires Rust extension for Postcard\n",
    "\n",
    "### Further Exploration\n",
    "\n",
    "- **Hybrid search**: Combine semantic search with BM25 for keyword matching\n",
    "- **Reranking**: Use cross-encoder models to rerank results\n",
    "- **Query expansion**: Generate related queries automatically\n",
    "- **Faceted search**: Add filters by domain, language, date, etc.\n",
    "- **Knowledge graph**: Integrate DBpedia/Wikidata entities\n",
    "- **Clustering**: Group similar pages together\n",
    "- **Trend analysis**: Analyze what content is indexed over time\n",
    "\n",
    "### Resources\n",
    "\n",
    "- GitHub repository: https://github.com/wilsonzlin/datasets/search-engine-open-data/\n",
    "- Original blog post: https://blog.wilsonl.in/search-engine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}